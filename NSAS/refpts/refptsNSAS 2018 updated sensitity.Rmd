---
output: 
  word_document:
    reference_docx: ../../_Common/report_template_v1.5.dotx
---

```{r setup, include=FALSE}
# ==============================================================================
# North Sea herring; final assessment at benchmark meeting 2018
# Sensititivity analysis with ricker and segreg
#
# Uses: R 3.4.3
#       FLCore 2.6.5
#       msy 0.1.18
#
# 22/02/2018 Martin Pastoors
# 17/03/2018 Checked and updated folders after cleanup of github
# 18/05/2019 Adapted for the new msy package (replaces eqsr_fit_shift)
# 20/05/2019 from r to Rmd
# ==============================================================================

require("knitr")
knitr::opts_chunk$set(echo = FALSE,	message = FALSE,	warning = FALSE,	comment = "",	crop = TRUE )
knitr::opts_chunk$set(fig.width=10) 

# rm(list=ls())

#library(devtools)  ## install.packages("devtools")
library(FLCore)
library(FLSAM)
library(msy)       ## install_github("ices-tools-prod/msy")
library(tidyverse)
library(captioner)

# setwd("D:/Repository/ICES_HAWG/wg_HAWG/NSAS/benchmark/")
# try(setwd("NSAS/refpts"),silent=FALSE)

# source("../refpts/Refpoints functions.R")
# source("../../_Common/eqsr_fit_shift.R")
# load("./results/7_finalModel/NSH_final.RData")

source("Refpoints functions.R")
# source("../../_Common/eqsr_fit_shift.R")

# Settings for captioner
fig_nums <- captioner::captioner(prefix = "Figure")
tab_nums <- captioner::captioner(prefix = "Table")

source("../../../mptools/R/my_utils.r")

# Smooth hockey function adapted to Mesnil
Smooth_hockey <- function(a, ssb) smooth_hockey(a, ssb, gamma = sqrt(0.001 * 4))

# Segreg model with Blim breakpoint and (roughly) geomean rec above this
SegregBlim  <- function(ab, ssb) log(ifelse(ssb >= Blim, ab$a * Blim, ab$a * ssb))

# load data; for sharepoint NEED TO OPEN THE SHAREPOINT FOLDER IN EXPLORER FIRST!!
load("//community.ices.dk@SSL/DavWWWRoot/ExpertGroups/benchmarks/2018/wkherring/2014 Meeting docs/06. Data/NSAS/SAM/NSH_final.RData")

# settings
final.year       <- dims(NSH)$maxyear
bio.years        <- c((final.year-9):final.year)
sel.years        <- c((final.year-9):final.year)
recruitment.trim <- c(3, -3)
trunc.years      <- c(2002:final.year)
fcv              <- 0.24; fphi    <- 0.40
# fcv              <- 0.24; fphi   <- 0.51
nsamp            <- 200


```

**Sensitivity analysis of reference points for North Sea herring, following the 2018 benchmark**

**Martin Pastoors^1^**

`r format(Sys.time(), '%d/%m/%Y %H:%M')`

[ All the codes to generate this document are in _refptsNSAS 2018 updated sensitivity.Rmd_ on the wgHAWG github site]

Biological reference points using the development version of ICES MSY package (v.0.1.18) and EQSIM method, applied to the final assessment of the WKPELA Benchmark 2018. 

This review was triggered by the outcomes of the MSE analysis on North Sea herring that was carried out in the spring of 2019. This work that was based on full feedback simulation models, gave rise to a different perception of reference points like Fmsy compared to the outcomes of the 2018 benchmark that was based on an EQSIM analysis. The new estimate of sustainable harvest rate, respecting the risk to Blim, in the MSE came out at 0.23 or 0.24, whereas the EQSIM analysis in 2018 came out at 0.26. Was this difference due to larger uncertainty taken into account in the full-feedback simulations? To test that, I wanted to go back to the EQSIM analysis in 2018 and explore the effects of different assumptions on uncertainty. 

In going back to the 2018 reference point estimation code, I quickly discovered that there were perhaps some small glitches in the code used to estimate the reference points ( _refptsNSAS 2018.r_ ): 1) in step 5 Blim was set to 1 000 000 tonnes instead of 900 000 as had been estimated above, 2) the was fcv and fphi had been derived was perhaps not fully consistent with the guidelines in WKMSYREF4 (because of a different number of years being used and of using the standard deviation on F rather than the CV on F).

This lead to a correction of the spreadsheet to estimate Fcv and Fphi based either on the status quo F in the forecast or to the recommended catch ( _Herring NS FCV and phi - Corrected.xlsx_ ).  For the reference points estimated in 2018 this meant a change of Fcv from 0.27 to 0.24, while Fphi was the same at 0.51. Running the reference point estimation with these updated values and the correct value for Blim, did not lead to a change in estimated reference points ( _see code below_ ).   


```{r echo=TRUE, fig.align="center", fig.asp=0.6, message=FALSE, warning=FALSE}

# 1. Get estimate of Blim using the whole time series and calculate Bpa

# FIT_segregBlim <- eqsr_fit_shift(NSH,nsamp=2000, models = "Segreg", rshift=1)
FIT_segregBlim <- eqsr_fit(NSH,nsamp=2000, models = "Segreg", rshift=1)
blim <- round(FIT_segregBlim$sr.det$b/1e5)*1e5  # 796 kT = 800 kT

#Now calculate the uncertainty in SSB in terminal year. We need the sd that belongs to log(ssb) to calculate Bpa
logssb    <- subset(ssb(NSH.sam),year==2016)
sdmin     <- function(sdestim){
  return(abs(0.025 - dnorm(log(logssb$lbnd),log(logssb$value),sdestim)))}
sdSSB     <- optimize(sdmin,interval=c(1e-4,0.2))$minimum
Bpa       <- blim * exp(1.645*sdSSB)  # MP 878 kT
Bpa       <- ceiling(Bpa/1e5)*1e5  # rounding up


# 2. parameterize the segreg model with Blim breakpoint and (roughly) geomean rec above this
SegregBlim  <- function(ab, ssb) log(ifelse(ssb >= blim, ab$a * blim, ab$a * ssb))

# 3. truncate the NSH object
NSHtrunc <- trim(NSH, year=2002:2016)

# 4. fit the stock recruitment model(s)
FIT <- eqsr_fit(NSHtrunc, nsamp = 2000, models = c("Ricker", "SegregBlim"), rshift=1)

# 5. Get Flim and thereby Fpa. Run EqSim with no MSY Btrigger (i.e. run EqSim with Btrigger=0), and Fcv=Fphi=0
SIM <- eqsim_run(FIT,
                 bio.years = c(2007:2016),
                 bio.const = FALSE,
                 sel.years = c(2007:2016),
                 sel.const = FALSE,
                 recruitment.trim = c(3, -3),
                 Fcv       = 0,
                 Fphi      = 0,
                 # Blim      = 800000, Bpa = 1000000,    # From WKPELA 2018
                 Blim      = blim,   Bpa = Bpa,        # Corrected 20/5/2019
                 Btrigger  = 0,
                 Fscan     = seq(0,0.80,len=40),
                 verbose   = FALSE,
                 extreme.trim=c(0.01,0.99))


Flim      <- SIM$Refs2["catF","F50"]   # MP: 0.341

# Now calculate the uncertainty in F in terminal year. We need the sd that belongs to log(F) to calculate Fpa
logfbar   <- subset(fbar(NSH.sam),year==2016)
sdmin     <- function(sdestim){
             return(abs(0.025 - dnorm(log(logfbar$lbnd),log(logfbar$value),sdestim)))}
sdF       <- optimize(sdmin,interval=c(1e-4,0.2))$minimum
Fpa       <- Flim * exp(-1.645*sdF) #0.294  MP: 0.298     

# 6. Run EqSim with no MSY Btrigger (i.e. run EqSim with Btrigger=0),
#    to get initial FMSY ; if this initial FMSY value is > Fpa, reduce it to Fpa
SIM <- eqsim_run(FIT,
                 bio.years = c(2007:2016),
                 bio.const = FALSE,
                 sel.years = c(2007:2016),
                 sel.const = FALSE,
                 recruitment.trim = c(3, -3),
                 # Fcv       = 0.27,  Fphi  = 0.51,            #From 2018 WKPELA
                 Fcv       = 0.24,  Fphi  = 0.51,            #Corrected 20/5/2019
                 Blim      = blim,
                 Bpa       = Bpa,
                 Btrigger  = 0,
                 Fscan     = seq(0,0.80,len=40),
                 verbose   = FALSE,
                 extreme.trim=c(0.01,0.99))

Fmsy      <- SIM$Refs2["lanF","medianMSY"] #0.275
Fmsy      <- ifelse(Fmsy>Fpa,Fpa,Fmsy)


# Select MSY Btrigger   (from schematic guidelines: yes, yes, no -> 5th percentile of MSYBtrigger
MSYBtrigger <- SIM$Refs2["catB","F05"]  # MP 1396 kT
MSYBtrigger <- round((2*MSYBtrigger)/1e5)*1e5/2 # rounding

# 7. Check if FMSY is precautionary, so do a scan
checkFmsy <- eqsim_run(FIT,
           bio.years = c(2007:2016),
           bio.const = FALSE,
           sel.years = c(2007:2016),
           sel.const = FALSE,
           recruitment.trim = c(3, -3),
           # Fcv       = 0.27,  Fphi  = 0.51,            #From 2018 WKPELA
           Fcv       = 0.24,  Fphi  = 0.51,            #Corrected 20/5/2019
           Blim      = blim,
           Bpa       = Bpa,
           Btrigger  = MSYBtrigger,
           Fscan     = seq(0.18,0.35,0.01),
#           Fscan     = seq(0.18,0.5,0.01),
           verbose   = FALSE,
           extreme.trim=c(0.01,0.99))

# If the precautionary criterion (FMSY < Fp.05) evaluated is not met, then FMSY should be reduced to  Fp.05. 
Fp05      <- checkFmsy$Refs2["catF","F05"] # MP: 0.256
propFmsy  <- subset(checkFmsy$pProfile,Ftarget==round(Fmsy,2) & variable=="Blim")$value
if (Fmsy > Fp05) {Fmsy <- Fp05}

Flim       <- round(Flim,2)
Fpa        <- round(Fpa,2)
Fmsy       <- round(Fmsy, 2)

# 8. final set of reference points
refpts <- data.frame(Flim       = Flim,
                     Fpa        = Fpa,
                     Fmsy       = Fmsy,
                     Blim       = blim,
                     Bpa        = Bpa,
                     MSYBtrigger= MSYBtrigger)

print(refpts)

```

The further improvement in the implementation of the Fcv/Fphi spreadsheet has now a built in evaluation of Fcv and Fphi using different number of years and using different end years, so as to evaluate the variability in the estimates ( _NSAS FCV and phi updated 20190520.xlsx_ ). For the benchmark assessment of 2018, this would have meant a Fcv = 0.25 and a Fphi = 0.41. 

Next, I started exploring the sensitivity to the variables Fcv and Fphi. I used 200 iterations instead of 2000, to save some time in the calculations, but they can easily be upscaled to 2000 if needed. I then run the basic operations until step 5 (estimating Flim and Fpa). From there I generated two loops, one with Fcv from 0.1 to 0,8 (including the estimated value of 0.25) and the second for Fphi from 0.1 to 0.8 (including the estimated value 0.41). Fmsy was not constrained by Fp05 in this calculation. They are simply both reported as outputs of the calculations.  

Blim and Bpa estimated with two significant numbers (instead of 1)

```{r echo=FALSE, fig.align="center", fig.asp=0.6, message=FALSE, warning=FALSE}

# settings
final.year       <- dims(NSH)$maxyear
bio.years        <- c((final.year-9):final.year)
sel.years        <- c((final.year-9):final.year)
recruitment.trim <- c(3, -3)
trunc.years      <- c(2002:final.year)
fcv              <- 0.24; fphi    <- 0.40  # values taken from "NSAS FCV and phi updated 20190520.xlsx""
nsamp            <- 200

# 1. Get estimate of Blim using the whole time series and calculate Bpa

FIT_segregBlim <- 
  eqsr_fit(NSH,nsamp=nsamp, models = "Segreg", rshift=1)
# eqsr_plot(FIT_segregBlim,n=2e4, ggPlot=FALSE)

blim <- round(FIT_segregBlim$sr.det$b/1e4)*1e4  # 796 kT = 800 kT

#Now calculate the uncertainty in SSB in terminal year. We need the sd that belongs to log(ssb) to calculate Bpa
logssb    <- subset(ssb(NSH.sam),year==final.year)
sdmin     <- function(sdestim){
  return(abs(0.025 - dnorm(log(logssb$lbnd),log(logssb$value),sdestim)))}
sdSSB     <- optimize(sdmin,interval=c(1e-4,0.2))$minimum
bpa       <- blim * exp(1.645*sdSSB)  # MP 878 kT
bpa       <- ceiling(bpa/1e4)*1e4  # rounding up

# 2. parameterize the segreg model with Blim breakpoint and (roughly) geomean rec above this
SegregBlim  <- function(ab, ssb) log(ifelse(ssb >= blim, ab$a * blim, ab$a * ssb))

# 3. truncate the NSH object
NSHtrunc <- trim(NSH, year=trunc.years)

# 4. fit the stock recruitment model(s)
FIT <- eqsr_fit(NSHtrunc, nsamp = nsamp, models = c("Ricker", "SegregBlim"), rshift=1)

# 5. Get Flim and thereby Fpa. Run EqSim with no MSY Btrigger (i.e. run EqSim with Btrigger=0), and Fcv=Fphi=0
SIM <- eqsim_run(FIT,
                 bio.years        = bio.years,
                 bio.const        = FALSE,
                 sel.years        = sel.years,
                 sel.const        = FALSE,
                 recruitment.trim = recruitment.trim,
                 Fcv              = 0,
                 Fphi             = 0,
                 Blim             = blim,
                 Bpa              = bpa,
                 Btrigger         = 0,
                 Fscan            = seq(0,0.80,len=40),
                 verbose          = TRUE,
                 extreme.trim     = c(0.01,0.99))

Flim      <- SIM$Refs2["catF","F50"]   # MP: 0.341

# Now calculate the uncertainty in F in terminal year. We need the sd that belongs to log(F) to calculate Fpa
logfbar   <- subset(fbar(NSH.sam),year==final.year)
sdmin     <- function(sdestim){
             return(abs(0.025 - dnorm(log(logfbar$lbnd),log(logfbar$value),sdestim)))}
sdF       <- optimize(sdmin,interval=c(1e-4,0.2))$minimum
Fpa       <- Flim * exp(-1.645*sdF) #0.294  MP: 0.298     

# set up loops to evaluate effect of uncertainties
df <- data.frame()

for (fcv in c(0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)) {
  for (fphi in (c(0.1, 0.2, 0.3, 0.4, 0.41, 0.5, 0.6, 0.7, 0.8))) {

    # 6. Run EqSim with no MSY Btrigger (i.e. run EqSim with Btrigger=0),
    #    to get initial FMSY ; if this initial FMSY value is > Fpa, reduce it to Fpa
    SIM <- eqsim_run(FIT,
                     bio.years        = bio.years,
                     bio.const        = FALSE,
                     sel.years        = sel.years,
                     sel.const        = FALSE,
                     recruitment.trim = recruitment.trim,
                     Fcv              = fcv,
                     Fphi             = fphi,
                     Blim             = blim,
                     Bpa              = bpa,
                     Btrigger         = 0,
                     Fscan            = seq(0,0.80,len=40),
                     verbose          = FALSE,
                     extreme.trim     = c(0.01,0.99))
    
    Fmsy      <- SIM$Refs2["lanF","medianMSY"] #0.275
    Fmsy      <- ifelse(Fmsy>Fpa,Fpa,Fmsy)
    
    
    # Select MSY Btrigger   (from schematic guidelines: yes, yes, no -> 5th percentile of biomass at Fmsy
    MSYBtrigger <- SIM$Refs2["catB","F05"]  # MP 1396 kT
    MSYBtrigger <- round((2*MSYBtrigger)/1e5)*1e5/2 # rounding
    
    # 7. Check if FMSY is precautionary, so do a scan
    checkFmsy <- eqsim_run(FIT,
                           bio.years        = bio.years,
                           bio.const        = FALSE,
                           sel.years        = sel.years,
                           sel.const        = FALSE,
                           recruitment.trim = recruitment.trim,
                           Fcv              = fcv,
                           Fphi             = fphi,
                           Blim             = blim,
                           Bpa              = bpa,
                           Btrigger         = MSYBtrigger,
                           Fscan            = seq(0.18,0.35,0.01),
                           verbose          = FALSE,
                           extreme.trim     = c(0.01,0.99))

    # If the precautionary criterion (FMSY < Fp.05) evaluated is not met, then FMSY should be reduced to  Fp.05. 
    Fp05      <- checkFmsy$Refs2["catF","F05"] # MP: 0.256
    propFmsy  <- subset(checkFmsy$pProfile,Ftarget==round(Fmsy,2) & variable=="Blim")$value
    # if (Fmsy > Fp05) {Fmsy <- Fp05}
    
    # 8. final set of reference points
    refpts <- data.frame(Flim       = Flim,
                         Fpa        = Fpa,
                         Fmsy       = Fmsy,
                         Fp05       = Fp05,
                         Blim       = blim,
                         Bpa        = bpa,
                         MSYBtrigger= MSYBtrigger) %>% 
      mutate(fcv = fcv, 
             fphi = fphi)
    
    df <- bind_rows(df, refpts) 
    
    # print(refpts)
    
  }
}

```

The first plot shows the values of Fmsy and Fp05 relative to Fcv (x-axis) and Fphi (colours). An increase in Fcv leads to a decrease in both Fmsy and Fp05. Note that when the uncertainty is very large (> 0.6), the calculation of Fp05 often does falls over, leading to NA values. The dashed lines indicate the values that are calculated as Fcv (0.25) and Fphi (0.41) from the benchmark assessment, which is associated with Fmsy and Fp05 of 0.29. The target fishing mortalities that are calculated from the MSE process are in the order of 0.23, which would be linked with Fcv ranging from 0.35 when Fphi is 0.8, to 0.65 when Fphi is 0.1. Overall this suggests that the uncertainty in the MSE evaluation is substantially larger than estimated from the previous 10 years of assessments and advice. 

```{r echo=FALSE, fig.align="center", fig.asp=0.5, message=FALSE, warning=FALSE}

t <- data.frame(variable=c("Fmsy", "Fp05"),
                fcv     =c(0.25, 0.25),
                value   =c(filter(df, fcv==0.25, fphi==0.41)$Fmsy, filter(df, fcv==0.25, fphi==0.41)$Fp05))

df %>% 
  tidyr::gather(key=variable, value=value, Fmsy:Fp05) %>% 
  ggplot(aes(x=fcv, y=value)) +
  theme_bw() +
  theme(legend.position="bottom") +
  geom_line(aes(colour=as.character(fphi))) +
  geom_point(aes(colour=as.character(fphi))) +
  
  geom_segment(data=t, aes(x=fcv, xend=fcv, y=0, yend=value), linetype="dashed") +
  geom_segment(data=t, aes(x=0, xend=fcv, y=value, yend=value), linetype="dashed") +
  
  geom_hline(yintercept=0.23, linetype = "dotted") +
  
  labs(y="F") +
  
  # expand_limits(y=0) +
  facet_wrap(~variable)

```

The next plot shows the Fphi in the panels and the difference between Fmsy and Fp05 in the colours, clearly showing that in most cases Fp05 will be constraining the value of Fmsy being adopted. 

```{r echo=FALSE, fig.align="center", fig.asp=0.8, message=FALSE, warning=FALSE}

df %>% 
  tidyr::gather(key=variable, value=value, Fmsy:Fp05) %>% 
  mutate(fphi = paste0("Fphi=",fphi)) %>% 
  ggplot(aes(x=fcv, y=value)) +
  theme_bw() +
  theme(legend.position="bottom") +
  geom_line(aes(colour=variable)) +
  geom_point(aes(colour=variable)) +
  labs(y="F") +
  
  # expand_limits(y=0) +
  facet_wrap(~fphi)

```


**Discussion**

The analysis on reference points for North Sea herring was carried out to better understand the implied uncertainties in the MSE for North Sea herring that was recently carried out by ICES at the request of the coastal states. That evaluation resulted in a target fishing mortality around 0.23 for different scenarios. This is lower than the reference points that were calculated at the WKPELA benchmark of 2018 for North Sea herring (Fmsy = 0.26). It is unlikely that this could be due to the specific management plans being evaluated, because most of the plans that show similarity to the standard ICES MSY rule, also showed comparable results in target fishing mortality. I then hypothesized that this could be due to the full-feedback simulations and the level of uncertainty that it was generating. The basic premise being that a higher uncertainty will lead to a larger probability of the stock going below Blim and therefore a lower target F being needed to keep the probability below 5%. 

When going back to the reference points estimated at WKPELA 2018, I discovered a number of small glitches in the way the Fcv and Fphi had been estimated, and how it had been implemented in the code. When fixing these small glitches, they were found not to impact the estimated reference points. 

However, the way Fcv and Fphi had been estimated differed somewhat from the approach in WKMSYREF4, in the sense that the number of years being used had increased whereas in WKMSYREF4 a 10 year period had been used. I then changed the spreadsheet to calculate of Fcv and Fphi of North Sea herring, by including an option to set the number of years being used and the end year of the data ( _NSAS FCV and phi updated 20190520.xlsx_ ). In this way it is now possible to assess the change in parameters of Fcv and Fphi. On the basis of the benchmark assessment, Fcv ranged from 0.23 to 0.28 and Fphi from 0.41 and 0.53. Generally, I would recommend to better describe and implement the methods (and their derivation) for estimating Fcv and Fphi in a technical guidelines document for reference points (currently Fphi and Fcv is only mentioned once).  

The evaluation of the sensitivity of the calculated values for Fmsy and Fp05 to the uncertainty in the assessments was carried out by creating a loop over Fcv and Fphi from 0.1 to 0.8. There is a clear relationship between uncertainty, autocorrelation and Fmsy and Fp05. The higher the uncertainty and the higher the autocorrelation, the lower Fmsy and Fp05, whereby the Fp05 calculation breaks down if the uncertainty gets too high. 

The is a substantial difference between the uncertainty that is estimated from the assessment and advice, and the apparent uncertainty that is an emergent property of the MSE evaluations with the resulting target fishing mortality of 0.23. In order to be able to understand the results of the MSE, it is recommended to better study the uncertainty that is embedded in the similations and how they relate to the assessments both now and in the past. Overall, the process of MSE evaluations is intended to explore the potential effects of certain management choices, under some assumed scenarios for biological and fisheries dynamics. To explain those effects, it is a requirement that the a coherent story can be told on the processes that lead to the observed phenomena. Using simpler tools than the full feedback MSE could be a way to get a better handle on those processes. 


